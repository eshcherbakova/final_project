{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rank_bm25 import BM25Okapi\n",
    "from multiprocessing import Pool,Lock\n",
    "import tqdm\n",
    "import catboost\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lg\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "queries = {}\n",
    "with open('./normal_queries.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "        titles= {}\n",
    "with open(\"./normal_titles.txt\" ,'r', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]\n",
    "\n",
    "train=[]\n",
    "train_titles = []\n",
    "train_queries = []\n",
    "trq = set()\n",
    "with open(\"./train.marks.tsv/train.marks.tsv\", 'r') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split('\\t')\n",
    "        if line[1] in titles.keys():\n",
    "            train.append([line[0],line[1], line[2]])\n",
    "            train_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in trq:\n",
    "                train_queries.append([line[0], queries[line[0]]])\n",
    "                trq.add(line[0])\n",
    "\n",
    "                \n",
    "test=[]\n",
    "test_titles = []\n",
    "test_queries = []\n",
    "teq = set()\n",
    "with open(\"./sample.csv/sample.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split(',')\n",
    "        if line[1] in titles.keys():\n",
    "            test.append([line[0],line[1],-1])\n",
    "            test_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in teq:\n",
    "                test_queries.append([line[0], queries[line[0]]])\n",
    "                teq.add(line[0])\n",
    "\n",
    "alq = list(train) + list(test)\n",
    "dctqs = {}\n",
    "for q, t, n in alq:\n",
    "    if q not in dctqs.keys():\n",
    "        dctqs[q] = []\n",
    "    dctqs[q].append(t)\n",
    "alqueries = train_queries + test_queries\n",
    "id_q = {}\n",
    "for el in alqueries:\n",
    "    id_q[el[0]] = el[1]   \n",
    "\n",
    "altits = {}\n",
    "for el in train_titles + test_titles:\n",
    "    altits[el[0]] = el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "import sentencepiece\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "outdata = \"features/luse\"\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "def cos(sp1,sp2):\n",
    "    #sp1 = sp1.todense()\n",
    "    #sp2 = sp2.todense()\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n",
    "\n",
    "import time\n",
    "def getsimuse(qid):\n",
    "    raw_tits = {}\n",
    "    global id_q\n",
    "    global altits\n",
    "    global embed\n",
    "    #t1 = time.time()\n",
    "    for dc in dctqs[qid]:\n",
    "        try:\n",
    "            raw_tits[dc] = altits[dc]\n",
    "        except:\n",
    "            print(qid, dc)\n",
    "    q = embed([id_q[qid]]).numpy()\n",
    "    embs = embed(list(raw_tits.values())).numpy()\n",
    "    #t2 = time.time()\n",
    "    #print(t2 - t1)\n",
    "    results = []\n",
    "    with open('features/luse/{}.txt'.format(qid),'w') as fout:\n",
    "        for i,doc in enumerate(raw_tits.keys()):\n",
    "            fout.write(str(qid) + '\\t' +str(doc) + '\\t' + str(cos(q,embs[i])) + '\\n')\n",
    "    #print(time.time() - t2)\n",
    "    del embs\n",
    "    del q\n",
    "    del raw_tits\n",
    "    return results\n",
    "\n",
    "for q in tqdm.tqdm(list(dctqs.keys())[2891:]):\n",
    "    r = getsimuse(q)\n",
    "lfiles = os.listdir(outdata)\n",
    "with open('features/luse.txt','w') as fout:\n",
    "    for file in lfiles:\n",
    "        with open(outdata+file) as fin:\n",
    "            line = fin.readline()\n",
    "            while line != '':\n",
    "                fout.write(line)\n",
    "                line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_altits = {}\n",
    "\n",
    "for k in tqdm.tqdm(altits.keys()):\n",
    "    embed_altits[k] = embed(altits[k]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_obj = open(\"embed_altits.pickle\",\"wb\")\n",
    "pickle.dump(embed_altits, pickle_obj)\n",
    "pickle_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_obj = open(\"embed_altits.pickle\", \"rb\")\n",
    "embed_altits_pickle = pickle.load(pickle_obj)\n",
    "pickle_obj.close()\n",
    "embed_altits_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "import sentencepiece\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "outdata = \"features/luse/\"\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "def cos(sp1,sp2):\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n",
    "\n",
    "import time\n",
    "def getsimuse(qid):\n",
    "    raw_tits = {}\n",
    "    global id_q\n",
    "    global altits\n",
    "    global embed\n",
    "    global embed_altits\n",
    "    embs = []\n",
    "    for dc in dctqs[qid]:\n",
    "        try:\n",
    "            raw_tits[dc] = altits[dc]\n",
    "            embs.append(embed_altits[dc])\n",
    "        except:\n",
    "            print(qid, dc)\n",
    "    q = embed([id_q[qid]]).numpy()\n",
    "    results = []\n",
    "    with open('features/luse/{}.txt'.format(qid),'w') as fout:\n",
    "        for i,doc in enumerate(raw_tits.keys()):\n",
    "            fout.write(str(qid) + '\\t' +str(doc) + '\\t' + str(cos(q,embs[i])) + '\\n')\n",
    "    del embs\n",
    "    del q\n",
    "    del raw_tits\n",
    "    return results\n",
    "\n",
    "for q in tqdm.tqdm(list(dctqs.keys())):\n",
    "    r = getsimuse(q)\n",
    "lfiles = os.listdir(outdata)\n",
    "with open('features/luse.txt','w') as fout:\n",
    "    for file in lfiles:\n",
    "        with open(outdata+file) as fin:\n",
    "            line = fin.readline()\n",
    "            while line != '':\n",
    "                fout.write(line)\n",
    "                line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtitles = [TaggedDocument(doc, [i]) for i, doc in train_titles]\n",
    "dqueries = [TaggedDocument(doc, [i]) for i, doc in train_queries]\n",
    "dstitles = [TaggedDocument(doc, [i]) for i, doc in test_titles]\n",
    "dsqueries = [TaggedDocument(doc, [i]) for i, doc in test_queries]\n",
    "#docvec similarity on queries\n",
    "from nltk.tokenize import word_tokenize\n",
    "aq = [TaggedDocument(doc,[i]) for i, doc in alqueries]\n",
    "dv = Doc2Vec(aq, vector_size=128, window=2, min_count=1, workers=4)\n",
    "max_epochs = 100\n",
    "alpha = 0.0025\n",
    "dv = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "dv.build_vocab(aq)\n",
    "for epoch in range(max_epochs):\n",
    "    dv.train(aq,\n",
    "    total_examples=dv.corpus_count,\n",
    "    epochs=dv.iter)\n",
    "    # decrease the learning rate\n",
    "    dv.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    dv.min_alpha = dv.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvecs = {}\n",
    "for el in alqueries:\n",
    "    dvecs[el[0]] = dv.docvecs[el[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(sp1,sp2):\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simdoc = {}\n",
    "def getsim(key):\n",
    "    global dvecs\n",
    "    tmp = {}\n",
    "    for tkey in dvecs.keys():\n",
    "        if tkey != key:\n",
    "            tmp[tkey] = cos(dvecs[key],dvecs[tkey])\n",
    "    sortres = sorted(tmp.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    simdoc[key] = []\n",
    "    res = []\n",
    "    for i in range(20):\n",
    "        res.append(sortres[i][0])\n",
    "    return [key,res]\n",
    "\n",
    "sims = {}\n",
    "for key in tqdm.tqdm(dvecs.keys()):\n",
    "    sims[key] = getsim(key)[1]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/prevsimdoc.txt','w') as fout:\n",
    "    for key in sims.keys():\n",
    "        fout.write(key+'\\t')\n",
    "        for el in sims[key]:\n",
    "            fout.write(el+' ')\n",
    "        fout.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiliggual USE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "import sentencepiece\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_altits_multiligual = {}\n",
    "\n",
    "for k in tqdm.tqdm(altits.keys()):\n",
    "    embed_altits_multiligual[k] = embed(altits[k]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_obj = open(\"embed_altits_multiligual.pickle\",\"wb\")\n",
    "pickle.dump(embed_altits_multiligual, pickle_obj)\n",
    "pickle_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_obj = open(\"embed_altits_multiligual.pickle\", \"rb\")\n",
    "embed_altits_multiligual_pickle = pickle.load(pickle_obj)\n",
    "pickle_obj.close()\n",
    "embed_altits_multiligual_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(sp1,sp2):\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n",
    "\n",
    "import time\n",
    "def getsimuse(qid):\n",
    "    raw_tits = {}\n",
    "    global id_q\n",
    "    global altits\n",
    "    global embed\n",
    "    global embed_altits_multiligual\n",
    "    embs = []\n",
    "    for dc in dctqs[qid]:\n",
    "        try:\n",
    "            raw_tits[dc] = altits[dc]\n",
    "            embs.append(embed_altits_multiligual[dc])\n",
    "        except:\n",
    "            print(qid, dc)\n",
    "    q = embed([id_q[qid]]).numpy()\n",
    "    results = []\n",
    "    with open('./features/muse/{}.txt'.format(qid),'w') as fout:\n",
    "        for i,doc in enumerate(raw_tits.keys()):\n",
    "            fout.write(str(qid) + '\\t' +str(doc) + '\\t' + str(cos(q,embs[i])) + '\\n')\n",
    "    del embs\n",
    "    del q\n",
    "    del raw_tits\n",
    "    return results\n",
    "for q in tqdm.tqdm(dctqs.keys()):\n",
    "    r = getsimuse(q)\n",
    "mfiles = os.listdir('./features/muse/')\n",
    "with open('./features/muse.txt','w') as fout:\n",
    "    for file in mfiles:\n",
    "        with open('./features/muse/'+file) as fin:\n",
    "            line = fin.readline()\n",
    "            while line != '':\n",
    "                fout.write(line)\n",
    "                line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docvec similarity on queries\n",
    "from nltk.tokenize import word_tokenize\n",
    "#dq = [TaggedDocument(doc,[i]) for i, doc in train_titles + test_titles]\n",
    "\n",
    "lst = []\n",
    "for key in dctqs.keys():    \n",
    "    for el in dctqs[key]:\n",
    "        lst.append([el,altits[el]])\n",
    "    \n",
    "dq = [TaggedDocument(doc,[i]) for i, doc in lst]\n",
    "for key in dctqs.keys():\n",
    "    dq.append(TaggedDocument(id_q[key],[1111111]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_titles + test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "alpha = 0.0025\n",
    "dv = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "dv.build_vocab(dq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm.tqdm(range(max_epochs)):\n",
    "        dv.train(dq,\n",
    "        total_examples=dv.corpus_count,\n",
    "        epochs=dv.iter)\n",
    "        dv.alpha -= 0.0002\n",
    "        dv.min_alpha = dv.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_obj = open(\"dv.pickle\",\"wb\")\n",
    "pickle.dump(dv, pickle_obj)\n",
    "pickle_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(sp1,sp2):\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docvec similarity on titles\n",
    "def docdst(key):\n",
    "    lst = []\n",
    "    global dctqs\n",
    "    global altits\n",
    "    global id_q\n",
    "    global dv\n",
    "    with open('./features/dv/{}.txt'.format(key),'w', encoding = 'utf-8') as fout:\n",
    "          for el in dctqs[key]:\n",
    "                fout.write(str(key) + '\\t' + str(el) + '\\t' + str(cos(dv.docvecs[1111111],dv.docvecs[el]))+'\\n')\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm.tqdm(dctqs.keys()):\n",
    "    docdst(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mfiles = os.listdir('./features/dv/')\n",
    "\n",
    "with open('./features/dvs.txt','w', encoding = 'utf-8') as fout:\n",
    "    for file in mfiles:\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open('./features/dv/'+file, \"r\", encoding = 'utf-8') as fin:\n",
    "                line = fin.readline()\n",
    "                while line != '':\n",
    "                    fout.write(line)\n",
    "                    line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext trained\n",
    "import fasttext.util\n",
    "import fasttext\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "model = FastText(min_count=1, size = 10, workers=8, negative=5, iter=10, min_n=2, max_n=10)\n",
    "def getsim(q):\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    global altits\n",
    "    raw = {}\n",
    "    model = FastText(min_count=1, size = 10, workers=2, negative=3, iter=10, min_n=2, max_n=9)\n",
    "    for doc in dctqs[q]:\n",
    "        try:\n",
    "            raw[doc] = word_tokenize(altits[doc])\n",
    "        except:\n",
    "            continue\n",
    "    raw['-1'] = word_tokenize(id_q[q])\n",
    "    model.build_vocab(raw.values())\n",
    "    model.train(raw.values(), total_examples=model.corpus_count, epochs=10)\n",
    "    with open('./features/trft/{}.txt'.format(q),'w') as fout:\n",
    "        for outdoc in raw.keys():\n",
    "            if outdoc != '-1':\n",
    "                fout.write(str(outdoc)+ '\\t'+str(cos(model.wv[altits[outdoc].replace(' ','_')], model.wv[id_q[q].replace(' ','_')])) + '\\n')\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm.tqdm(dctqs.keys()):\n",
    "    getsim(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import os\n",
    "import operator\n",
    "from multiprocessing import Pool,Lock         \n",
    "#data \n",
    "queries = {}\n",
    "with open('./normal_queries.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "        titles= {}\n",
    "with open(\"./normal_titles.txt\" ,'r', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]\n",
    "\n",
    "train=[]\n",
    "train_titles = []\n",
    "train_queries = []\n",
    "trq = set()\n",
    "with open(\"./train.marks.tsv/train.marks.tsv\", 'r') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split('\\t')\n",
    "        if line[1] in titles.keys():\n",
    "            train.append([line[0],line[1], line[2]])\n",
    "            train_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in trq:\n",
    "                train_queries.append([line[0], queries[line[0]]])\n",
    "                trq.add(line[0])\n",
    "\n",
    "                \n",
    "test=[]\n",
    "test_titles = []\n",
    "test_queries = []\n",
    "teq = set()\n",
    "with open(\"./sample.csv/sample.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split(',')\n",
    "        if line[1] in titles.keys():\n",
    "            test.append([line[0],line[1],-1])\n",
    "            test_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in teq:\n",
    "                test_queries.append([line[0], queries[line[0]]])\n",
    "                teq.add(line[0])\n",
    "\n",
    "alq = list(train) + list(test)\n",
    "dctqs = {}\n",
    "for q, t, n in alq:\n",
    "    if q not in dctqs.keys():\n",
    "        dctqs[q] = []\n",
    "    dctqs[q].append(t)\n",
    "alqueries = train_queries + test_queries\n",
    "id_q = {}\n",
    "for el in alqueries:\n",
    "    id_q[el[0]] = el[1]   \n",
    "\n",
    "altits = {}\n",
    "for el in train_titles + test_titles:\n",
    "    altits[el[0]] = el[1]\n",
    "def cos(sp1,sp2):\n",
    "    sp1 = sp1.todense()\n",
    "    sp2 = sp2.todense()\n",
    "    try:\n",
    "        res = np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n",
    "    except:\n",
    "        res = None\n",
    "    return res\n",
    "try:\n",
    "    os.mkdir('./features/ttfsim')\n",
    "except:\n",
    "    n = None\n",
    "def ptfidfsim(q):\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    global altits\n",
    "    tf = TfidfVectorizer(analyzer = 'char', ngram_range=(3,9))\n",
    "    raw = {}\n",
    "    for doc in dctqs[q]:\n",
    "        try:\n",
    "            raw[doc] = altits[doc]\n",
    "        except:\n",
    "            continue\n",
    "    raw['-1'] = id_q[q]\n",
    "    vecs = tf.fit_transform(raw.values())\n",
    "    with open('./features/ttfsim/{}.txt'.format(q),'w') as fout:\n",
    "        for i,outdoc in enumerate(raw.keys()):\n",
    "            if outdoc != '-1':\n",
    "                fout.write(str(outdoc)+ '\\t'+str(cos(vecs[i],vecs[-1])) + '\\n')\n",
    "    return\n",
    "\n",
    "def pLtfidfsim(q):\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    global altits\n",
    "    tf = TfidfVectorizer(analyzer = 'char', ngram_range=(1,3))\n",
    "    raw = {}\n",
    "    for doc in dctqs[q]:\n",
    "        try:\n",
    "            raw[doc] = altits[doc]\n",
    "        except:\n",
    "            continue\n",
    "    raw['-1'] = id_q[q]\n",
    "    vecs = tf.fit_transform(raw.values())\n",
    "    with open('./features/ttfsim/{}.txt'.format(q),'w') as fout:\n",
    "        for i,outdoc in enumerate(raw.keys()):\n",
    "            if outdoc != '-1':\n",
    "                fout.write(str(outdoc)+ '\\t'+str(cos(vecs[i],vecs[-1])) + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q,t in tqdm.tqdm(alqueries):\n",
    "    ptfidfsim(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "#data \n",
    "queries = {}\n",
    "with open('./normal_queries.txt', encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "        titles= {}\n",
    "with open(\"./normal_titles.txt\" ,'r', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]\n",
    "\n",
    "train=[]\n",
    "train_titles = []\n",
    "train_queries = []\n",
    "trq = set()\n",
    "with open(\"./train.marks.tsv/train.marks.tsv\", 'r', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split('\\t')\n",
    "        if line[1] in titles.keys():\n",
    "            train.append([line[0],line[1], line[2]])\n",
    "            train_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in trq:\n",
    "                train_queries.append([line[0], queries[line[0]]])\n",
    "                trq.add(line[0])\n",
    "\n",
    "                \n",
    "test=[]\n",
    "test_titles = []\n",
    "test_queries = []\n",
    "teq = set()\n",
    "with open(\"./sample.csv/sample.csv\", 'r', encoding='utf-8') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split(',')\n",
    "        if line[1] in titles.keys():\n",
    "            test.append([line[0],line[1],-1])\n",
    "            test_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in teq:\n",
    "                test_queries.append([line[0], queries[line[0]]])\n",
    "                teq.add(line[0])\n",
    "\n",
    "alq = list(train) + list(test)\n",
    "dctqs = {}\n",
    "for q, t, n in alq:\n",
    "    if q not in dctqs.keys():\n",
    "        dctqs[q] = []\n",
    "    dctqs[q].append(t)\n",
    "alqueries = train_queries + test_queries\n",
    "id_q = {}\n",
    "for el in alqueries:\n",
    "    id_q[el[0]] = el[1]   \n",
    "\n",
    "altits = {}\n",
    "for el in train_titles + test_titles:\n",
    "    altits[el[0]] = el[1]\n",
    "\n",
    "outdata = './features/pasT/'\n",
    "def getpscore(qid):\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    global altits\n",
    "    texts = {}\n",
    "    idfdict = {}\n",
    "    for doc in dctqs[qid]:\n",
    "        try:\n",
    "            texts[doc] = word_tokenize(altits[doc])\n",
    "            for w in texts[doc]:\n",
    "                if w not in idfdict.keys():\n",
    "                    idfdict[w] = 0\n",
    "                idfdict[w] += 1\n",
    "        except:\n",
    "            continue\n",
    "    for key in idfdict.keys():\n",
    "        idfdict[key] = len(texts) / idfdict[key]\n",
    "    \n",
    "    query = word_tokenize(id_q[qid])\n",
    "    qset = set(query)\n",
    "    with open(outdata+qid+'.txt','w') as fout:\n",
    "        for key in texts.keys():\n",
    "            ltext = texts[key]\n",
    "            pas = 0\n",
    "            for i in range(len(ltext) - 2):\n",
    "                scut = set(ltext[i:i+2])\n",
    "                ints = set.intersection(scut,qset)\n",
    "                if len(ints) > 0:\n",
    "                    cidf = 0\n",
    "                    for el in ints:\n",
    "                        cidf+=idfdict[el]\n",
    "                    for el in ints:\n",
    "                        for k in ints:\n",
    "                            if el != k:\n",
    "                                if (ltext.index(el) - ltext.index(k))*(query.index(el) - query.index(k)) > 0:\n",
    "                                    cidf *= 1.1\n",
    "                                if abs((ltext.index(el) - ltext.index(k)))<=abs((query.index(el) - query.index(k))):\n",
    "                                    cidf *= 1.05\n",
    "                    pas+=cidf*(len(ltext) - i)\n",
    "            fout.write(str(key) + '\\t' + str(pas) + '\\n')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in tqdm.tqdm(dctqs.keys()):\n",
    "    getpscore(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata = './features/pas3/'\n",
    "def getpsmallscore(qid):\n",
    "    if qid+'.txt' in os.listdir(outdata):\n",
    "        return\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    texts = {}\n",
    "    idfdict = {}\n",
    "    for doc in dctqs[qid]:\n",
    "        try:\n",
    "            with open(data+doc+'.txt') as fin:\n",
    "                texts[doc] = word_tokenize(fin.readline())\n",
    "                for w in texts[doc]:\n",
    "                    if w not in idfdict.keys():\n",
    "                        idfdict[w] = 0\n",
    "                    idfdict[w] += 1\n",
    "        except:\n",
    "            continue\n",
    "    for key in idfdict.keys():\n",
    "        idfdict[key] = len(texts) / idfdict[key]\n",
    "    query = word_tokenize(id_q[qid])\n",
    "    qset = set(query)\n",
    "    with open(outdata+qid+'.txt','w') as fout:\n",
    "        for key in texts.keys():\n",
    "            ltext = texts[key]\n",
    "            pas = 0\n",
    "            intl = []\n",
    "            for i in range(0,len(ltext) - 3,2):\n",
    "                scut = set(ltext[i:i+3])\n",
    "                ints = set.intersection(scut,qset)\n",
    "                intl.append(ints)\n",
    "                if len(ints) > 0:\n",
    "                    cidf = 0\n",
    "                    for el in ints:\n",
    "                        cidf+=idfdict[el]\n",
    "                    for el in ints:\n",
    "                        for k in ints:\n",
    "                            if el != k:\n",
    "                                if (ltext.index(el) - ltext.index(k))*(query.index(el) - query.index(k)) > 0:\n",
    "                                    cidf *= 1.1\n",
    "                                if abs((ltext.index(el) - ltext.index(k)))<=abs((query.index(el) - query.index(k))):\n",
    "                                    cidf *= 1.05\n",
    "                    pas+=cidf*(len(ltext) - 3 - i)\n",
    "            fout.write(str(key) + '\\t' + str(pas) + '\\n')\n",
    "    return\n",
    "\n",
    "outdata = './features/pas/'\n",
    "def getpscore(qid):\n",
    "    if qid+'.txt' in os.listdir(outdata):\n",
    "        return\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    texts = {}\n",
    "    idfdict = {}\n",
    "    for doc in dctqs[qid]:\n",
    "        try:\n",
    "            with open(data+doc+'.txt') as fin:\n",
    "                texts[doc] = word_tokenize(fin.readline())\n",
    "                for w in texts[doc]:\n",
    "                    if w not in idfdict.keys():\n",
    "                        idfdict[w] = 0\n",
    "                    idfdict[w] += 1\n",
    "        except:\n",
    "            continue\n",
    "    for key in idfdict.keys():\n",
    "        idfdict[key] = len(texts) / idfdict[key]\n",
    "    query = word_tokenize(id_q[qid])\n",
    "    qset = set(query)\n",
    "    with open(outdata+qid+'.txt','w') as fout:\n",
    "        for key in texts.keys():\n",
    "            ltext = texts[key]\n",
    "            pas = 0\n",
    "            intl = []\n",
    "            for i in range(0,len(ltext) - 7,3):\n",
    "                scut = set(ltext[i:i+7])\n",
    "                ints = set.intersection(scut,qset)\n",
    "                intl.append(ints)\n",
    "                if len(ints) > 0:\n",
    "                    cidf = 0\n",
    "                    for el in ints:\n",
    "                        cidf+=idfdict[el]\n",
    "                    for el in ints:\n",
    "                        for k in ints:\n",
    "                            if el != k:\n",
    "                                if (ltext.index(el) - ltext.index(k))*(query.index(el) - query.index(k)) > 0:\n",
    "                                    cidf *= 1.1\n",
    "                                if abs((ltext.index(el) - ltext.index(k)))<=abs((query.index(el) - query.index(k))):\n",
    "                                    cidf *= 1.05\n",
    "                    pas+=cidf*(len(ltext) - 7 - i)\n",
    "            fout.write(str(key) + '\\t' + str(pas) + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in tqdm.tqdm(dctqs.keys()):\n",
    "    getpscore(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in tqdm.tqdm(dctqs.keys()):\n",
    "    getpsmallscore(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "from pyaspeller import YandexSpeller\n",
    "import pymorphy2\n",
    "import locale\n",
    "from pymystem3 import Mystem\n",
    "from multiprocessing import Pool\n",
    "from p_tqdm import p_imap\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "import scipy.spatial.distance as ds\n",
    "import tqdm\n",
    "from p_tqdm import p_map\n",
    "import fasttext.util\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = YandexSpeller(lang='en', find_repeat_words=False,\n",
    "                                       ignore_digits=True, max_requests=555)\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def stem(line):\n",
    "    l = line.split('\\t')\n",
    "    text = l[1].strip()\n",
    "    qid=l[0]\n",
    "    m = Mystem()\n",
    "    lemmas = m.lemmatize(text.lower())\n",
    "    return qid,''.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "pool = Pool(6)\n",
    "file = open(\"normal_queries.txt\", 'w', encoding = 'utf-8')\n",
    "with open(\"qspell.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    lines= f.readlines()\n",
    "    for qid, norm_text in p_map(stem,lines):\n",
    "        file.write(qid+'\\t'+norm_text)\n",
    "        queries[qid] = norm_text\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_queries.txt\" ,'r', encoding='utf-8')\n",
    "queries = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    queries[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"qspell.txt\" ,'r', encoding='utf-8')\n",
    "queries_not_normal = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    queries_not_normal[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"title.txt\" ,'r', encoding='utf-8')\n",
    "titles_not_normal= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles_not_normal[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_titles.txt\" ,'r', encoding='utf-8')\n",
    "titles= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"train.marks.tsv/train.marks.tsv\", 'r')\n",
    "train=[]\n",
    "train_map={}\n",
    "queries_doc_map_all={}\n",
    "marks={}\n",
    "for l in tqdm.tqdm(file.readlines()):\n",
    "    splits=l.split('\\t')\n",
    "    if int(splits[2])==0:\n",
    "        continue\n",
    "    train.append([int(splits[0]),int(splits[1]), int(splits[2])])\n",
    "    marks[splits[0]+\"|\"+splits[1]]=int(splits[2])\n",
    "    train_map[splits[0]+'|'+splits[1]]=splits[2].strip()\n",
    "    if int(splits[0]) not in queries_doc_map_all.keys():\n",
    "        queries_doc_map_all[int(splits[0])]=[int(splits[1])]\n",
    "    else:\n",
    "        queries_doc_map_all[int(splits[0])].append(int(splits[1]))\n",
    "train=np.array(train)\n",
    "file.close()\n",
    "test_map={}\n",
    "file = open(\"sample.csv/sample.csv\", 'r')\n",
    "file.readline()\n",
    "test=[]\n",
    "for l in tqdm.tqdm(file.readlines()):\n",
    "    splits=l.split(',')\n",
    "    test.append([int(splits[0]),int(splits[1]), -1])\n",
    "    test_map[splits[0]+'|'+splits[1].strip()]=\"-1\"\n",
    "    if int(splits[0]) not in queries_doc_map_all.keys():\n",
    "        queries_doc_map_all[int(splits[0])]=[int(splits[1])]\n",
    "    else:\n",
    "        queries_doc_map_all[int(splits[0])].append(int(splits[1]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi, BM25Plus, BM25L\n",
    "from stop_words import get_stop_words\n",
    "stop = get_stop_words('ru')+get_stop_words('en')+['aren', 'can', \n",
    "    'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll',\n",
    "        'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n=1, word=True):\n",
    "    if word:\n",
    "        n_grams = ngrams(word_tokenize(text), n)\n",
    "        return [ ' '.join(grams) for grams in n_grams]\n",
    "    else:\n",
    "        n_grams = ngrams(text, n)\n",
    "        return [ ''.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_titles(doc_ids, bm=False, n=1, word=True, doc2vec=False):\n",
    "    for i in doc_ids:\n",
    "        try:\n",
    "            if bm:\n",
    "                yield get_ngrams(titles[i], n=n, word=word)\n",
    "            elif doc2vec:\n",
    "                yield i,titles[i]\n",
    "            else:\n",
    "                yield titles[i]\n",
    "        except:\n",
    "            if doc2vec:\n",
    "                yield i,\"0\"\n",
    "            else:\n",
    "                yield \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_titles_tr(doc_ids, bm=False, n=1, word=True, doc2vec=False):\n",
    "    for i in doc_ids:\n",
    "        try:\n",
    "            if bm:\n",
    "                yield get_ngrams(tr_titles[i], n=n, word=word)\n",
    "            elif doc2vec:\n",
    "                yield i,tr_titles[i]\n",
    "            else:\n",
    "                yield tr_titles[i]\n",
    "        except:\n",
    "            if doc2vec:\n",
    "                yield i,\"0\"\n",
    "            else:\n",
    "                yield \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(filename, features):\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        for i in features.keys():\n",
    "            f.write(str(i)+'\\t')\n",
    "            for j in features[i]:\n",
    "                if not np.isfinite(j) or j<0:\n",
    "                    j=0\n",
    "                f.write(str(j)+'\\t')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_doc(qid):\n",
    "    doc={}\n",
    "    f=open(\"docs/{}.txt\".format(qid), 'r', encoding='utf-8')\n",
    "    for line in f.readlines():\n",
    "        l=line.split('\\t')\n",
    "        doc[int(l[0])]=l[1].strip()\n",
    "    f.close()\n",
    "    for i in queries_doc_map_all[qid]:\n",
    "        if i in doc.keys():\n",
    "            yield doc[i]\n",
    "        else:\n",
    "            yield \"not found\"\n",
    "    del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tfidf={}\n",
    "for i in tqdm.tqdm(range(0,len(queries_doc_map_all))):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=None)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos9=cosine(X,q).ravel()\n",
    "    cos1=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=stop)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos2=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos10=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos3=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos11=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4,13), stop_words=stop, analyzer='char_wb')\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos4=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos12=cosine(X,q).ravel()\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1),use_idf=False,stop_words=None)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos5=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos13=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=stop,use_idf=False)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos6=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos14=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop, use_idf=False)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos7=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos15=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4,13), stop_words=stop, analyzer='char_wb', use_idf=False)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos8=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos16=cosine(X,q).ravel()\n",
    "      \n",
    "    \n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=1))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res1=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=2))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res2=bm25.get_scores(get_ngrams(queries[i], n=2, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=3, word=False))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res3=bm25.get_scores(get_ngrams(queries[i], n=3, word=False))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=5, word=False))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res4=bm25.get_scores(get_ngrams(queries[i], n=5, word=False))\n",
    "    \n",
    "    \n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=1))\n",
    "    bm25 = BM25L(sents)\n",
    "    res5=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=2))\n",
    "    bm2 = BM25L(sents)\n",
    "    res6=bm25.get_scores(get_ngrams(queries[i], n=2, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=3, word=False))\n",
    "    bm25 = BM25L(sents)\n",
    "    res7=bm25.get_scores(get_ngrams(queries[i], n=3, word=False))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=5, word=False))\n",
    "    bm25 = BM25L(sents)\n",
    "    res8=bm25.get_scores(get_ngrams(queries[i], n=5, word=False))\n",
    "    \n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=1))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res9=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=2))\n",
    "    bm2 = BM25Okapi(sents)\n",
    "    res10=bm25.get_scores(get_ngrams(queries[i], n=2, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=3, word=False))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res11=bm25.get_scores(get_ngrams(queries[i], n=3, word=False))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=5, word=False))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res12=bm25.get_scores(get_ngrams(queries[i], n=5, word=False))\n",
    "    \n",
    "    for k,j in enumerate(queries_doc_map_all[i]):\n",
    "        features_tfidf[str(i)+\"|\"+str(j)]=[cos1[k],cos2[k], cos3[k],cos4[k],cos5[k],cos6[k], cos7[k],cos8[k],\n",
    "                       cos9[k],cos10[k], cos11[k],cos12[k],cos13[k],cos14[k], cos15[k],cos16[k],res1[k], res2[k], res3[k], res4[k],\n",
    "                                              res5[k], res6[k], res7[k], res8[k], res9[k], res10[k], res11[k], res12[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(filename, features):\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        for i in features.keys():\n",
    "            f.write(str(i)+'\\t')\n",
    "            for j in features[i]:\n",
    "                if not np.isfinite(j) or j<0:\n",
    "                    j=0\n",
    "                f.write(str(j)+'\\t')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features('features/tfidf.txt', features_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import gensim\n",
    "import zipfile\n",
    "\n",
    "import fasttext.util\n",
    "model1=gensim.models.KeyedVectors.load(\"/Users/lmac/Downloads/181/model.model\")\n",
    "model2=gensim.models.KeyedVectors.load(\"/Users/lmac/Downloads/187/model.model\")\n",
    "ft_en = fasttext.load_model('/Users/lmac/Downloads/cc.en.300.bin')\n",
    "ft_ru = fasttext.load_model('/Users/lmac/Downloads/cc.ru.300.bin')\n",
    "ft_new= fasttext.load_model('/Users/lmac/Downloads/ft_native_300_ru_wiki_lenta_lemmatize.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(qid):\n",
    "    res=[]\n",
    "    q_vector1=np.zeros(300)\n",
    "    q_vector2=np.zeros(300)\n",
    "    q_vector3=np.zeros(300)\n",
    "    q_vector4=np.zeros(300)\n",
    "    q_vector8=np.zeros(300)\n",
    "    q_vector5=ft_ru.get_sentence_vector(queries[qid])\n",
    "    q_vector6=ft_en.get_sentence_vector(queries[qid])\n",
    "    q_vector7=ft_new.get_sentence_vector(queries[qid])\n",
    "    for w in queries[qid].split():\n",
    "        q_vector1+=model1.get_vector(w)\n",
    "        q_vector2+=model2.get_vector(w)\n",
    "        q_vector3+=ft_ru.get_word_vector(w)\n",
    "        q_vector4+=ft_en.get_word_vector(w)\n",
    "        q_vector8+=ft_new.get_word_vector(w)\n",
    "    for title in list(generator_titles(queries_doc_map_all[qid])):\n",
    "        d_vector1=np.zeros(300)\n",
    "        d_vector2=np.zeros(300)\n",
    "        d_vector3=np.zeros(300)\n",
    "        d_vector4=np.zeros(300)\n",
    "        d_vector8=np.zeros(300)\n",
    "        d_vector5=ft_ru.get_sentence_vector(title)\n",
    "        d_vector6=ft_en.get_sentence_vector(title)\n",
    "        d_vector7=ft_new.get_sentence_vector(title)\n",
    "        for w in title.split():\n",
    "            d_vector1+=model1.get_vector(w)\n",
    "            d_vector2+=model2.get_vector(w)\n",
    "            d_vector3+=ft_ru.get_word_vector(w)\n",
    "            d_vector4+=ft_en.get_word_vector(w)\n",
    "            d_vector8+=ft_new.get_word_vector(w)\n",
    "        s=[model1.similarity(queries[qid], title),\n",
    "        model2.similarity(queries[qid], title),\n",
    "        ds.cosine(q_vector1,d_vector1),\n",
    "        ds.cosine(q_vector2,d_vector2),\n",
    "        ds.cosine(q_vector3,d_vector3),\n",
    "        ds.cosine(q_vector4,d_vector4),\n",
    "        ds.cosine(q_vector5,d_vector5),\n",
    "        ds.cosine(q_vector6,d_vector6),\n",
    "        ds.cosine(q_vector7,d_vector7),\n",
    "        ds.cosine(q_vector8,d_vector8),\n",
    "        np.dot(q_vector1,d_vector1),\n",
    "        np.dot(q_vector2,d_vector2),\n",
    "        np.dot(q_vector3,d_vector3),\n",
    "        np.dot(q_vector4,d_vector4),\n",
    "        np.dot(q_vector5,d_vector5),\n",
    "        np.dot(q_vector6,d_vector6),\n",
    "        np.dot(q_vector7,d_vector7),\n",
    "        np.dot(q_vector8,d_vector8)]\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fasttext={}\n",
    "for qid in tqdm.tqdm(range(6311)):\n",
    "    for i,j in zip(queries_doc_map_all[qid], sim(qid)):\n",
    "        features_fasttext[str(qid)+\"|\"+str(i)]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features(\"features/fasttext.txt\", features_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_obj = open(\"features_fasttext.pickle\",\"wb\")\n",
    "pickle.dump(features_fasttext, pickle_obj)\n",
    "pickle_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_titles.txt\" ,'r', encoding='utf-8')\n",
    "titles= []\n",
    "id_titles=[]\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles.append(l[1].strip())\n",
    "    id_titles.append(int(l[0]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "model1 = SentenceTransformer('distiluse-base-multilingual-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_from_model1=model1.encode(titles, show_progress_bar=True, batch_size=128)\n",
    "titles_={}\n",
    "for i,j in zip(id_titles,vectors_from_model1):\n",
    "    titles_[i]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features/bert.txt\", 'w', encoding='utf-8') as f:\n",
    "    for qid in tqdm.tqdm(range(len(queries))):\n",
    "        q_emb=model1.encode([queries[qid]])[0]\n",
    "        for i in queries_doc_map_all[qid]:\n",
    "            if i in titles_.keys():\n",
    "                f.write(str(qid)+\"|\"+str(i)+'\\t'+str(ds.cosine(q_emb, titles_[i]))+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\", use_fast=True)\n",
    "model1 = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\", use_fast=True)\n",
    "model2 = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\", use_fast=True)\n",
    "model3 = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_bert(qid):\n",
    "    res=[]\n",
    "    device = 'cpu'\n",
    "    q_token = tokenizer1.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb1 = model1(**q_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "    q_token = tokenizer2.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb2 = model2(**q_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "    q_token = tokenizer3.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb3 = model3(**q_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "    for title in generator_titles(queries_doc_map_all[qid]):\n",
    "        d_token = tokenizer1.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb1 = model1(**d_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "        d_token = tokenizer2.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb2 = model2(**d_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "        d_token = tokenizer3.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb3 = model3(**d_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "        res.append([cosine(q_emb1[0], d_emb1[0]).mean(),cosine(q_emb1.sum(1), d_emb1.sum(1)).ravel()[0],\n",
    "                   cosine(q_emb2[0], d_emb2[0]).mean(),cosine(q_emb2.sum(1), d_emb2.sum(1)).ravel()[0],\n",
    "                   cosine(q_emb3[0], d_emb3[0]).mean(),cosine(q_emb3.sum(1), d_emb3.sum(1)).ravel()[0]])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_bert(qid):\n",
    "    global queries\n",
    "    global queries_doc_map_all\n",
    "    global generator_titles\n",
    "    \n",
    "    res=[]\n",
    "    device = 'cpu'\n",
    "    q_token = tokenizer1.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb1 = model1(**q_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "    q_token = tokenizer2.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb2 = model2(**q_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "    q_token = tokenizer3.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb3 = model3(**q_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "    for title in generator_titles(queries_doc_map_all[qid]):\n",
    "        d_token = tokenizer1.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb1 = model1(**d_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "        d_token = tokenizer2.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb2 = model2(**d_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "        d_token = tokenizer3.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb3 = model3(**d_token)[0].to(torch.device('cpu')).detach().numpy()\n",
    "        res.append([cosine(q_emb1[0], d_emb1[0]).mean(),cosine(q_emb1.sum(1), d_emb1.sum(1)).ravel()[0],\n",
    "                   cosine(q_emb2[0], d_emb2[0]).mean(),cosine(q_emb2.sum(1), d_emb2.sum(1)).ravel()[0],\n",
    "                   cosine(q_emb3[0], d_emb3[0]).mean(),cosine(q_emb3.sum(1), d_emb3.sum(1)).ravel()[0]])\n",
    "    with open('./features/deeppavlov/{}.txt'.format(qid),'w') as fout:\n",
    "        for i,j in zip(queries_doc_map_all[qid], res):\n",
    "            fout.write(str(qid)+\"|\"+str(i)+'\\t')\n",
    "            for k in j:\n",
    "                fout.write(str(k)+'\\t')\n",
    "            fout.write('\\n')\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open(\"features/transforms.txt\", 'a', encoding='utf-8')\n",
    "for qid in tqdm.tqdm(range(0,6311)):\n",
    "    for i,j in zip(queries_doc_map_all[qid], sim_bert(qid)):\n",
    "        file.write(str(qid)+\"|\"+str(i)+'\\t')\n",
    "        for k in j:\n",
    "            file.write(str(k)+'\\t')\n",
    "        file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "#embed1 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "#embed2 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "module3 = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_USE(qid):\n",
    "    questions = [queries_not_normal[qid]]\n",
    "    question_embeddings = module3.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "    responses = list(generator_titles(queries_doc_map_all[qid]))\n",
    "    response_contexts = responses\n",
    "    response_embeddings = module3.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "    sim1 = np.inner(question_embeddings['outputs'], response_embeddings['outputs']).ravel()\n",
    "    #sim2 = cosine(embed1(queries_not_normal[qid]), embed1(list(generator_titles_not_normal(queries_doc_map_all[qid])))).ravel()\n",
    "    #sim3 = cosine(embed2(queries_not_normal[qid]), embed2(list(generator_titles_not_normal(queries_doc_map_all[qid])))).ravel()\n",
    "    return np.hstack((sim1.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "features_USE_not_normal={}\n",
    "with open(\"features/USE_not_normal.txt\", 'a+') as f:\n",
    "    for qid in tqdm.tqdm(range(1203, 6311)):\n",
    "        for i,j in zip(queries_doc_map_all[qid], sim_USE(qid)):\n",
    "            features_USE_not_normal[str(qid)+\"|\"+str(i)]=j\n",
    "            f.write(str(qid)+\"|\"+str(i)+'\\t'+str(j)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_USE(207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import operator\n",
    "import lightgbm as lg\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "queries = {}\n",
    "with open('./normal_queries.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "titles= {}\n",
    "with open(\"./normal_titles.txt\" ,'r', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]\n",
    "\n",
    "train=[]\n",
    "with open(\"./train.marks.tsv/train.marks.tsv\", 'r') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split('\\t')\n",
    "        train.append([line[0],line[1], line[2]])\n",
    "\n",
    "                \n",
    "test=[]\n",
    "with open(\"./sample.csv/sample.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split(',')\n",
    "        test.append([line[0],line[1],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostdict = {}\n",
    "with open('./url.data/url.data') as fin:\n",
    "    lines = fin.readlines()\n",
    "for line in lines:\n",
    "    line = line[:-1].split('\\t')\n",
    "    if line[1][:7] == 'http://':\n",
    "        line[1] = line[1][7:]\n",
    "    if line[1][:4] == 'www.':\n",
    "        line[1] = line[1][4:]\n",
    "    hostdict[line[0]] = line[1].split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similars = {}\n",
    "with open('./features/prevsimdoc.txt', 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        spl = line[:-2].split('\\t')\n",
    "        if len(spl) == 1:\n",
    "            continue\n",
    "        tmp = spl[1].split(' ')\n",
    "        for i in range(13):\n",
    "            tmp.pop()\n",
    "        similars[spl[0]] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"./features/dvs.txt\" ,'r', encoding='utf-8')\n",
    "dvs= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    dvs[(l[0], l[1])]= np.array(l[2][:-1], dtype = np.float64)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pas.py\n",
    "pas = {}\n",
    "pasfiles = os.listdir('./features/pas')\n",
    "for file in pasfiles:\n",
    "    with open('./features/pas/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if len(line) > 1:\n",
    "            pas[(file[:-4], line[0])] = np.log(1 + np.float64(line[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse = {}\n",
    "musefiles = os.listdir('./features/muse')\n",
    "for file in musefiles:\n",
    "    with open('./features/muse/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if len(line) > 1:\n",
    "            muse[(line[0], line[1])] = np.float64(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = {}\n",
    "pastfiles = os.listdir('./features/pasT')\n",
    "for file in pastfiles:\n",
    "    with open('./features/pasT/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if len(line) > 1:\n",
    "            past[(file[:-4], line[0])] = np.log(1+ np.float64(line[1]))\n",
    "            \n",
    "pas3 = {}\n",
    "pas3files = os.listdir('./features/pas3')\n",
    "for file in pas3files:\n",
    "    with open('./features/pas3/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if len(line) > 1:\n",
    "            pas3[(file[:-4], line[0])] = np.log(1+ np.float64(line[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luse = {}\n",
    "with open('./features/luse.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        luse[(line[0], line[1])] = np.float64(line[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trft = {}\n",
    "trftfiles = os.listdir('./features/trft')\n",
    "for file in pastfiles:\n",
    "    with open('./features/trft/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if len(line) > 1:\n",
    "            trft[(file[:-4], line[0])] = np.float64(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"./features/DBN/part-r-00000\" ,'r', encoding='utf-8')\n",
    "dbndoc = {}\n",
    "for line in file.readlines():\n",
    "    l=line[:-1].split('\\t')\n",
    "    k = l[0]\n",
    "    l.pop(0)\n",
    "    dbndoc[k]= np.array(l, dtype = np.float64)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostfiles = os.listdir('./features/DDF')\n",
    "hf = {}\n",
    "qhf = {}\n",
    "c = 0\n",
    "for file in hostfiles:\n",
    "    with open('./features/DDF/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        line.pop()\n",
    "        if file[0] == 'Q':\n",
    "            q = line.pop(0)\n",
    "            h = line.pop(0)\n",
    "            qhf[(q,h)] = np.array(line, dtype = np.float64)\n",
    "            c+=1\n",
    "        else:\n",
    "            h = line.pop(0)\n",
    "            hf[h] = np.array(line, dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlfiles = os.listdir('./features/UDDF')\n",
    "uf = {}\n",
    "quf = {}\n",
    "for file in urlfiles:\n",
    "    with open('./features/UDDF/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        line.pop()\n",
    "        if file[1] == 'Q':\n",
    "            q = line.pop(0)\n",
    "            u = line.pop(0)\n",
    "            quf[(q,u)] = np.array(line, dtype = np.float64)\n",
    "        else:\n",
    "            u = line.pop(0)\n",
    "            uf[u] = np.array(line, dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"./features/QF/part-r-00000\" ,'r', encoding='utf-8')\n",
    "qf = {}\n",
    "for line in file.readlines():\n",
    "    l=line[:-1].split('\\t')\n",
    "    k = l[0]\n",
    "    l.pop(0)\n",
    "    qf[k]= np.array(l, dtype = np.float64)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quse = {}\n",
    "with open('./features/quse.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        quse[(line[0], line[1])] = np.float64(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(cat):\n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    qid_train = []\n",
    "    index = []\n",
    "    for entry in tqdm.tqdm(cat):\n",
    "        try:\n",
    "            try:\n",
    "                tempx = dvs[(entry[0],entry[1])]\n",
    "            except:\n",
    "                tempx = np.array([0.0])\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,trft[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,muse[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,past[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            trainx.append(tempx)\n",
    "            trainy.append(float(entry[2]))\n",
    "            qid_train.append(int(entry[0]))\n",
    "            index.append((entry[0], entry[1]))\n",
    "        except:\n",
    "            continue\n",
    "    Xtrain = np.row_stack([row for row in trainx])\n",
    "    Ytrain = np.row_stack([row for row in trainy])\n",
    "    return Xtrain, Ytrain, qid_train, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, qid_train, tindex = getdata(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(filename):\n",
    "    fet={}\n",
    "    with open(filename,'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            l=line.strip().split('\\t')\n",
    "            x=[]\n",
    "            for i in range(1, len(l)):\n",
    "                x.append(float(l[i].strip()))\n",
    "            fet[l[0]]=x\n",
    "    return fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tfidf=read_features(\"features/tfidf.txt\")\n",
    "features_fasttext=read_features(\"features/fasttext.txt\")\n",
    "features_bert=read_features(\"features/bert.txt\")\n",
    "features_transforms = read_features(\"features/transforms.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_hosts={}\n",
    "set_hosts =set()\n",
    "with open(\"url.data/url.data\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        l=line.split('\\t')\n",
    "        host=l[1].split(\"/\")[0]\n",
    "        host = host[4:] if host.startswith(\"www.\") else host\n",
    "        doc_id = int(l[0])\n",
    "        map_hosts[host]=doc_id\n",
    "        set_hosts.add(host)\n",
    "hosts={}\n",
    "with open(\"url.data/url.data\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        l=line.split('\\t')\n",
    "        host=l[1].split(\"/\")[0]\n",
    "        host = host[4:] if host.startswith(\"www.\") else host\n",
    "        host_id = map_hosts[host]    \n",
    "        doc_id = int(l[0])\n",
    "        if host_id in hosts.keys():\n",
    "            hosts[host_id].append(doc_id)\n",
    "        else:\n",
    "            hosts[host_id] = []\n",
    "            hosts[host_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_sDBN = {}\n",
    "file = open(\"features/new_sDBN/DOCS/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    d.append(float(splits[1]))\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    docs_sDBN[int(splits[0])]=np.array(d)\n",
    "file.close()\n",
    "qdocs_sDBN = {}\n",
    "file = open(\"features/new_sDBN/QDOCS/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    d.append(float(splits[4]))\n",
    "    qdocs_sDBN[splits[0]+\"|\"+splits[1]]=np.array(d)\n",
    "file.close()\n",
    "host_sDBN = {}\n",
    "file = open(\"features/new_sDBN/HOST/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    hid= int(splits[0])\n",
    "    d.append(float(splits[1]))\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    if hid in hosts.keys():\n",
    "        for host in hosts[hid]:\n",
    "            host_sDBN[host] =np.array(d)\n",
    "file.close()\n",
    "qhost_sDBN = {}\n",
    "file = open(\"features/new_sDBN/QHOST/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    hid= int(splits[1])\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    d.append(float(splits[4]))\n",
    "    if hid in hosts.keys():\n",
    "        for host in hosts[hid]:\n",
    "            if host in queries_doc_map_all[int(splits[0])]:\n",
    "                qhost_sDBN[splits[0]+\"|\"+str(host)]=np.array(d)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, Q=False):\n",
    "    Docs = {}\n",
    "    if Q:\n",
    "        start=2\n",
    "        end=13\n",
    "    else:\n",
    "        start=1\n",
    "        end=12\n",
    "    for i in range(11):\n",
    "        with open(\"features/new_PBM/\"+filename+\"/part-r-000\"+str(i).zfill(2), 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                data={}\n",
    "                l = line.strip().split('\\t')\n",
    "                for k in range(start,end):\n",
    "                    splt = l[k].strip().split(\":\")\n",
    "                    data[splt[0]]= float(splt[1])\n",
    "                for k in range(end,end+3):\n",
    "                    splt = l[k].split(\":\")\n",
    "                    pos = []\n",
    "                    for m,j in enumerate(splt[1].strip().split(\" \")):\n",
    "                        if m == 10:\n",
    "                            data[splt[0]+'_last']=int(j)\n",
    "                            break\n",
    "                        pos.append(int(j))\n",
    "                    data[splt[0]]=np.array(pos)\n",
    "                if Q:\n",
    "                    Docs[l[0]+'|'+l[1]]=data\n",
    "                else:\n",
    "                    Docs[int(l[0])]=data\n",
    "    return Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(Docs, Q=False,H=False):\n",
    "    features = {}\n",
    "    #alpha = np.arange(0.05, 0.45, 0.01)\n",
    "    alpha = np.array([0.41,0.16,0.105,0.08,0.06,0.05,0.35,0.3,0.25,0.2])\n",
    "    for i in Docs.keys():\n",
    "        #if Docs[i]['clicks']<=1:\n",
    "        #    continue\n",
    "        CTR = Docs[i]['clicks']/Docs[i]['shows']\n",
    "        AvgTime = Docs[i]['time']/Docs[i]['clicks'] if Docs[i]['clicks']!=0 else 0\n",
    "        FCTR = Docs[i]['first_click']/Docs[i]['shows']\n",
    "        LCTR = Docs[i]['last_click']/Docs[i]['shows']\n",
    "        AVGPos = np.mean(Docs[i]['shows_pos'])\n",
    "        AVGClick = np.mean(Docs[i]['click_pos'])\n",
    "        AVGPosClick = np.mean(Docs[i]['show_pos_if_click'])\n",
    "        Shows10 = Docs[i]['shows_pos_last']/Docs[i]['shows']\n",
    "        Clicks10 = Docs[i]['click_pos_last']/Docs[i]['clicks'] if Docs[i]['clicks']!=0 else 0\n",
    "        LastProb = Docs[i]['last_click']/Docs[i]['clicks'] if Docs[i]['clicks']!=0 else 0\n",
    "        NotFirst = np.sum(Docs[i]['click_pos'][1:])/Docs[i]['shows']\n",
    "        CTRPos = 1.*Docs[i]['show_pos_if_click']/Docs[i]['shows_pos'] #массив\n",
    "        Prob_Click_pos = Docs[i]['click_pos']/Docs[i]['clicks'] #массив\n",
    "        Prob_Click_showpos= Docs[i]['show_pos_if_click']/Docs[i]['clicks'] #массив\n",
    "        PBM = Docs[i]['clicks']/np.sum(Docs[i]['shows_pos']*alpha)\n",
    "        CM_shows =  Docs[i]['cm_clicks']/Docs[i]['cm_shows']  if Docs[i]['cm_shows']!=0 else 0\n",
    "        UpProb = Docs[i]['up_click']/Docs[i]['shows']\n",
    "        DownProb = Docs[i]['down_click']/Docs[i]['shows']\n",
    "        AVGbeforeShowDocs=Docs[i]['before_click']/(Docs[i]['clicks']-Docs[i]['first_click']) if (Docs[i]['clicks']-Docs[i]['first_click'])!=0 else 0\n",
    "        AVGafterShowDocs=Docs[i]['after_click']/(Docs[i]['clicks']-Docs[i]['last_click'])if (Docs[i]['clicks']-Docs[i]['last_click'])!=0 else 0\n",
    "        CTR5=np.sum(Docs[i]['show_pos_if_click'][0:5])/np.sum(Docs[i]['shows_pos'][0:5]) if np.sum(Docs[i]['shows_pos'][0:5])!=0 else 0\n",
    "        logShows =  np.log(1+Docs[i]['shows'])\n",
    "        logClicks =  np.log(1+Docs[i]['clicks'])\n",
    "        data = [CTR, AvgTime, FCTR, LCTR, AVGPos, AVGClick, AVGPosClick, Shows10, Clicks10,LastProb,AVGbeforeShowDocs,AVGafterShowDocs,CTR5,\n",
    "               NotFirst,PBM, CM_shows, logClicks, logShows,UpProb,DownProb]\n",
    "        for k in CTRPos:\n",
    "            if np.isnan(k):\n",
    "                k=0\n",
    "            data.append(k)\n",
    "        for k in Prob_Click_pos:\n",
    "            if np.isnan(k):\n",
    "                k=0\n",
    "            data.append(k)\n",
    "        for k in Prob_Click_showpos:\n",
    "            if np.isnan(k):\n",
    "                k=0\n",
    "            data.append(k)\n",
    "            \n",
    "            \n",
    "        if Q and H:\n",
    "            s=i.split('|')\n",
    "            if int(s[1]) in hosts.keys():\n",
    "                for host in hosts[int(s[1])]:\n",
    "                    if host in queries_doc_map_all[int(s[0])]:\n",
    "                        features[s[0]+\"|\"+ str(host)] = data\n",
    "        \n",
    "        if H and ~Q:\n",
    "            if i in hosts.keys():\n",
    "                for host in hosts[i]:\n",
    "                    features[host] = data \n",
    "        if ~H:\n",
    "            features[i] = data\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_make(filename, Q, H):\n",
    "    Docs=read_data(filename, Q)\n",
    "    return make_features(Docs,Q=Q,H=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_docs =read_and_make(\"DOCS\", Q=False, H=False)\n",
    "features_hosts =read_and_make(\"HOST\", Q=False, H=True)\n",
    "features_qdocs=read_and_make(\"QDOCS\", Q=True, H=False)\n",
    "features_qhosts =read_and_make(\"QHOST\", Q=True, H=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = {}\n",
    "with open(\"features/QUERY/part-r-00000\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        data={}\n",
    "        l = line.strip().split('\\t')\n",
    "        for k in range(1,len(l)):\n",
    "            splt = l[k].strip().split(\":\")\n",
    "            data[splt[0]]= float(splt[1])\n",
    "        Docs[int(l[0])]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_queries = {}\n",
    "for i in Docs.keys():\n",
    "    AVGDocs = Docs[i]['shows_docs']/Docs[i]['count_query']\n",
    "    AVGClicks = Docs[i]['clicks_docs']/Docs[i]['count_query']\n",
    "    AVGtime = Docs[i]['time']/Docs[i]['count_query'] if Docs[i]['time']<0 else 60*30\n",
    "    AVG_pos = Docs[i]['avg_pos_click']/Docs[i]['count_query']\n",
    "    AVG_pos_first = Docs[i]['first_click']/Docs[i]['count_query']\n",
    "    Noclick = Docs[i]['shows_noclick']\n",
    "    data=[AVGDocs,AVGClicks,AVGtime,AVG_pos,AVG_pos_first,Noclick]\n",
    "    features_queries[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(filename, features):\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        for i in features.keys():\n",
    "            f.write(str(i)+'\\t')\n",
    "            for j in features[i]:\n",
    "                if not np.isfinite(j) or j<0:\n",
    "                    j=0\n",
    "                f.write(str(j)+'\\t')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features(\"features/click_docs.txt\", features_docs)\n",
    "save_features(\"features/click_qdocs.txt\", features_qdocs)\n",
    "save_features(\"features/click_hosts.txt\", features_hosts)\n",
    "save_features(\"features/click_qhosts.txt\", features_qhosts)\n",
    "save_features(\"features/click_q.txt\", features_queries)\n",
    "save_features(\"features/sDBN_docs.txt\", docs_sDBN)\n",
    "save_features(\"features/sDBN_qdocs.txt\", qdocs_sDBN)\n",
    "save_features(\"features/sDBN_hosts.txt\", host_sDBN)\n",
    "save_features(\"features/sDBN_qhosts.txt\", qhost_sDBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(filename):\n",
    "    fet={}\n",
    "    with open(filename,'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            l=line.strip().split('\\t')\n",
    "            x=[]\n",
    "            for i in range(1, len(l)):\n",
    "                x.append(float(l[i].strip()))\n",
    "            fet[l[0]]=x\n",
    "    return fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_docs=read_features(\"features/click_docs.txt\") \n",
    "features_qdocs=read_features(\"features/click_qdocs.txt\")\n",
    "features_hosts=read_features(\"features/click_hosts.txt\")\n",
    "features_qhosts=read_features(\"features/click_qhosts.txt\")\n",
    "features_queries=read_features(\"features/click_q.txt\")\n",
    "docs_sDBN=read_features(\"features/sDBN_docs.txt\")\n",
    "qdocs_sDBN=read_features(\"features/sDBN_qdocs.txt\")\n",
    "host_sDBN=read_features(\"features/sDBN_hosts.txt\")\n",
    "qhost_sDBN=read_features(\"features/sDBN_qhosts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features(t):\n",
    "    features = {}\n",
    "    for i in t:\n",
    "        x1=features_tfidf[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_tfidf.keys() else np.zeros(len(features_tfidf['0|340485']))\n",
    "        x2=features_fasttext[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_fasttext.keys() else np.zeros(len(features_fasttext['0|6760']))\n",
    "        \n",
    "        x4=features_hosts[str(i[1])] if str(i[1]) in features_hosts.keys() else np.zeros(len(features_hosts['10059']))\n",
    "        x5=features_docs[str(i[1])] if str(i[1]) in features_docs.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x6=qdocs_sDBN[str(i[0])+\"|\"+str(i[1])]  if str(i[0])+\"|\"+str(i[1]) in qdocs_sDBN.keys() else np.zeros(3)\n",
    "        x7=qhost_sDBN[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in qhost_sDBN.keys() else np.zeros(3)\n",
    "        x8=docs_sDBN[str(i[1])] if str(i[1]) in docs_sDBN.keys() else np.zeros(3)\n",
    "        x9=host_sDBN[str(i[1])] if str(i[1]) in host_sDBN.keys() else np.zeros(3)\n",
    "        x10=features_qhosts[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_qhosts.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x11=features_qdocs[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_qdocs.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x12=features_queries[str(i[0])] if str(i[0]) in features_queries.keys() else np.zeros(6)        \n",
    "        \n",
    "        \n",
    "        x14=features_bert[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_bert.keys() else np.zeros(len(features_bert['0|340485']))\n",
    "        x15=features_transforms[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_transforms.keys() else np.zeros(len(features_transforms['0|340485']))\n",
    "        try:\n",
    "            x21 = dvs[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x21 = np.array([0.0])\n",
    "        try:    \n",
    "            x22 = trft[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x22 = np.array([0.0])\n",
    "        try:    \n",
    "            x23 = muse[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x23 = np.array([0.0])\n",
    "        try:\n",
    "            x24 = past[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x24 = np.array([0.0])\n",
    "        try:\n",
    "            x30 = texttf[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x30 = np.array([0.0])\n",
    "        try:\n",
    "            x31 = newbm[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x31 = np.array([0.0])           \n",
    "        try:\n",
    "            x25 = hf[hostdict[str(i[1])]]\n",
    "        except:\n",
    "\n",
    "            [x25] = [[0.0] * 13]\n",
    "        try:\n",
    "            x26 = qhf[(str(i[0]),hostdict[str(i[1])])]\n",
    "        except:\n",
    "\n",
    "            [x26] = [[0.0] * 43]\n",
    "\n",
    "        try:\n",
    "            x27 = uf[str(i[1])]\n",
    "        except:\n",
    "\n",
    "            [x27] = [[0.0] * 13]\n",
    "\n",
    "        try:\n",
    "            x28 = quf[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "\n",
    "            [x28] = [[0.0] * 43]\n",
    "        try:\n",
    "            x29 = dbndoc[str(i[1])]\n",
    "        except:\n",
    "\n",
    "            [x29] = [[0.0] * 3]            \n",
    "        \n",
    "        features[str(i[0])+\"|\"+str(i[1])+\"|\"+str(i[2])]=np.hstack((x1,x2,x14,x15, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features(t):\n",
    "    features = {}\n",
    "    for i in t:\n",
    "        x1=features_tfidf[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_tfidf.keys() else np.zeros(len(features_tfidf['0|340485']))\n",
    "        x2=features_fasttext[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_fasttext.keys() else np.zeros(len(features_fasttext['0|6760']))\n",
    "        \n",
    "        x4=features_hosts[str(i[1])] if str(i[1]) in features_hosts.keys() else np.zeros(len(features_hosts['10059']))\n",
    "        x5=features_docs[str(i[1])] if str(i[1]) in features_docs.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x6=qdocs_sDBN[str(i[0])+\"|\"+str(i[1])]  if str(i[0])+\"|\"+str(i[1]) in qdocs_sDBN.keys() else np.zeros(3)\n",
    "        x7=qhost_sDBN[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in qhost_sDBN.keys() else np.zeros(3)\n",
    "        x8=docs_sDBN[str(i[1])] if str(i[1]) in docs_sDBN.keys() else np.zeros(3)\n",
    "        x9=host_sDBN[str(i[1])] if str(i[1]) in host_sDBN.keys() else np.zeros(3)\n",
    "        x10=features_qhosts[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_qhosts.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x11=features_qdocs[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_qdocs.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x12=features_queries[str(i[0])] if str(i[0]) in features_queries.keys() else np.zeros(6)        \n",
    "        \n",
    "        x14=features_bert[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_bert.keys() else np.zeros(len(features_bert['0|340485']))\n",
    "        x15=features_transforms[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_transforms.keys() else np.zeros(len(features_transforms['0|340485']))\n",
    "        try:\n",
    "            x21 = dvs[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x21 = np.array([0.0])\n",
    "        try:    \n",
    "            x22 = trft[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x22 = np.array([0.0])\n",
    "        try:    \n",
    "            x23 = muse[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x23 = np.array([0.0])\n",
    "        try:\n",
    "            x24 = past[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x24 = np.array([0.0])\n",
    "        try:\n",
    "            x30 = quse[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x30 = np.array([0.0])         \n",
    "        try:\n",
    "            x25 = hf[hostdict[str(i[1])]]\n",
    "        except:\n",
    "\n",
    "            [x25] = [[0.0] * 13]\n",
    "        try:\n",
    "            x26 = qhf[(str(i[0]),hostdict[str(i[1])])]\n",
    "        except:\n",
    "\n",
    "            [x26] = [[0.0] * 43]\n",
    "\n",
    "        try:\n",
    "            x27 = uf[str(i[1])]\n",
    "        except:\n",
    "\n",
    "            [x27] = [[0.0] * 13]\n",
    "\n",
    "        try:\n",
    "            x28 = quf[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "\n",
    "            [x28] = [[0.0] * 43]\n",
    "        try:\n",
    "            x29 = dbndoc[str(i[1])]\n",
    "        except:\n",
    "\n",
    "            [x29] = [[0.0] * 3]\n",
    "            \n",
    "            \n",
    "        features[str(i[0])+\"|\"+str(i[1])+\"|\"+str(i[2])]=np.hstack((x1,x2,x4,x5,x6,x7,x8,x9,x10,x11,x12,x14,x15, x21, x22, x23, x24))\n",
    "\n",
    "        \n",
    "        #features[str(i[0])+\"|\"+str(i[1])+\"|\"+str(i[2])]=np.hstack((x1,x2,x14,x15, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features(t):\n",
    "    features = {}\n",
    "    for i in t:\n",
    "        x1=features_tfidf[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_tfidf.keys() else np.zeros(len(features_tfidf['0|340485']))\n",
    "        x2=features_fasttext[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_fasttext.keys() else np.zeros(len(features_fasttext['0|6760']))\n",
    "                \n",
    "        \n",
    "        x14=features_bert[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_bert.keys() else np.zeros(len(features_bert['0|340485']))\n",
    "        x15=features_transforms[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_transforms.keys() else np.zeros(len(features_transforms['0|340485']))\n",
    "        try:\n",
    "            x21 = dvs[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x21 = np.array([0.0])\n",
    "        try:    \n",
    "            x22 = trft[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x22 = np.array([0.0])\n",
    "        try:    \n",
    "            x23 = muse[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x23 = np.array([0.0])\n",
    "        try:\n",
    "            x24 = past[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x24 = np.array([0.0])\n",
    "        try:\n",
    "            x30 = quse[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x30 = np.array([0.0])\n",
    "        try:\n",
    "            x31 = texttf[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x31 = np.array([0.0])\n",
    "        try:\n",
    "            x32 = newbm[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "            x32 = np.array([0.0])            \n",
    "        try:\n",
    "            x25 = hf[hostdict[str(i[1])]]\n",
    "        except:\n",
    "\n",
    "            [x25] = [[0.0] * 13]\n",
    "        try:\n",
    "            x26 = qhf[(str(i[0]),hostdict[str(i[1])])]\n",
    "        except:\n",
    "\n",
    "            [x26] = [[0.0] * 43]\n",
    "\n",
    "        try:\n",
    "            x27 = uf[str(i[1])]\n",
    "        except:\n",
    "\n",
    "            [x27] = [[0.0] * 13]\n",
    "\n",
    "        try:\n",
    "            x28 = quf[(str(i[0]), str(i[1]))]\n",
    "        except:\n",
    "\n",
    "            [x28] = [[0.0] * 43]\n",
    "        try:\n",
    "            x29 = dbndoc[str(i[1])]\n",
    "        except:\n",
    "\n",
    "            [x29] = [[0.0] * 3]\n",
    "            \n",
    "            \n",
    "        #features[str(i[0])+\"|\"+str(i[1])+\"|\"+str(i[2])]=np.hstack((x1,x2,x4,x5,x6,x7,x8,x9,x10,x11,x12,x14,x15, x21, x22, x23, x24))\n",
    "\n",
    "        \n",
    "        features[str(i[0])+\"|\"+str(i[1])+\"|\"+str(i[2])]=np.hstack((x1,x2,x14,x15, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31, x32))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_queries.txt\" ,'r', encoding='utf-8')\n",
    "queries = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    queries[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"qspell.txt\" ,'r', encoding='utf-8')\n",
    "queries_not_normal = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    queries_not_normal[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"title.txt\" ,'r', encoding='utf-8')\n",
    "titles_not_normal= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles_not_normal[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_titles.txt\" ,'r', encoding='utf-8')\n",
    "titles= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"train.marks.tsv/train.marks.tsv\", 'r')\n",
    "train=[]\n",
    "train_map={}\n",
    "queries_doc_map_all={}\n",
    "marks={}\n",
    "for l in tqdm.tqdm(file.readlines()):\n",
    "    splits=l.split('\\t')\n",
    "    if int(splits[2])==0:\n",
    "        continue\n",
    "    train.append([int(splits[0]),int(splits[1]), int(splits[2])])\n",
    "    marks[splits[0]+\"|\"+splits[1]]=int(splits[2])\n",
    "    train_map[splits[0]+'|'+splits[1]]=splits[2].strip()\n",
    "    if int(splits[0]) not in queries_doc_map_all.keys():\n",
    "        queries_doc_map_all[int(splits[0])]=[int(splits[1])]\n",
    "    else:\n",
    "        queries_doc_map_all[int(splits[0])].append(int(splits[1]))\n",
    "train=np.array(train)\n",
    "file.close()\n",
    "test_map={}\n",
    "file = open(\"sample.csv/sample.csv\", 'r')\n",
    "file.readline()\n",
    "test=[]\n",
    "for l in tqdm.tqdm(file.readlines()):\n",
    "    splits=l.split(',')\n",
    "    test.append([int(splits[0]),int(splits[1]), -1])\n",
    "    test_map[splits[0]+'|'+splits[1].strip()]=\"-1\"\n",
    "    if int(splits[0]) not in queries_doc_map_all.keys():\n",
    "        queries_doc_map_all[int(splits[0])]=[int(splits[1])]\n",
    "    else:\n",
    "        queries_doc_map_all[int(splits[0])].append(int(splits[1]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train=all_features(train)\n",
    "features_test=all_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def make_data(features):\n",
    "    qid_data=[]\n",
    "    y_data = []\n",
    "    doc_ids=[]\n",
    "    X_data=[]\n",
    "    for l in features.keys():\n",
    "        line=l.split(\"|\")\n",
    "        qid_data.append(int(line[0]))\n",
    "        y_data.append(float(line[2]))\n",
    "        doc_ids.append(int(line[1]))\n",
    "        X_data.append(features[l])\n",
    "    qid_data=np.array(qid_data)\n",
    "    y_data=np.array(y_data)\n",
    "    X_data= csr_matrix(X_data)\n",
    "    doc_ids= np.array(doc_ids)\n",
    "    group_data =np.unique(qid_data, return_counts=True)[1]\n",
    "    return X_data, y_data, qid_data, group_data, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train, qid_train, group_train, doc_ids_train = make_data(features_train)\n",
    "X_test,y_test, qid_test, group_test, doc_ids_test = make_data(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as gbm\n",
    "\n",
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':100,\n",
    "'subsample':0.9,\n",
    "'learning_rate':0.1,\n",
    "'num_leaves':63,}\n",
    "Ranker1 = gbm.LGBMRanker(**params)\n",
    "Ranker1.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':500,\n",
    "'subsample':0.9,\n",
    "'learning_rate':0.1,\n",
    "'num_leaves':127,}\n",
    "Ranker2 = gbm.LGBMRanker(**params)\n",
    "Ranker2.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':1000,\n",
    "'subsample':0.95,\n",
    "'learning_rate':0.01,\n",
    "'num_leaves':31,}\n",
    "Ranker3 = gbm.LGBMRanker(**params)\n",
    "Ranker3.fit(X_train, y_train, group=group_train, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':3000,\n",
    "'subsample':0.95,\n",
    "'learning_rate':0.007,\n",
    "'num_leaves':29,}\n",
    "Ranker = gbm.LGBMRanker(**params)\n",
    "Ranker.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =(Ranker.predict(X_test)+Ranker1.predict(X_test)+Ranker2.predict(X_test)+Ranker3.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def save_submission(pred_qids, preds, all_doc_ids, filename):\n",
    "    fout = open(filename, 'w')\n",
    "    fout.write('QueryId,DocumentId\\n')\n",
    "    for qid in np.unique(pred_qids):\n",
    "        q_doc_idxs = np.argwhere(pred_qids == qid).ravel()\n",
    "        #print(q_doc_idxs)\n",
    "        doc_ids = copy.deepcopy(all_doc_ids[q_doc_idxs]).ravel()\n",
    "        #print(doc_ids)\n",
    "        q_doc_scores = preds[q_doc_idxs]\n",
    "        #print(q_doc_scores)\n",
    "        sorted_doc_ids = doc_ids[np.argsort(q_doc_scores)[::-1]]\n",
    "        for did in sorted_doc_ids:\n",
    "            fout.write('{0},{1}\\n'.format(qid, did))\n",
    "        \n",
    "    fout.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(qid_test,pred,doc_ids_test, 'test8.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as gbm\n",
    "lgrank = gbm.LGBMRanker(objective='lambdarank', max_depth=12, n_estimators= 1000, subsample=0.8, learning_rate=0.01, num_leaves=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgrank.fit(X_train, y_train, group=group_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgrank.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(qid_test,pred,doc_ids_test, 'test9.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "embed3 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "from deeppavlov import configs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deeppavlov.models.embedders.elmo_embedder import ELMoEmbedder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, alias = \"elmo_ru_news\", \"news\"\n",
    "model = ELMoEmbedder(str(prefix / \"ELMo/{}\".format(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "embed3 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module3 = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "module4= hub.load('https://tfhub.dev/google/universal-sentence-encoder-qa/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_USE(qid):\n",
    "    questions = [queries[qid]]\n",
    "    question_embeddings = module3.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "    responses = list(generator_titles(queries_doc_map_all[qid]))\n",
    "    response_contexts = list(generator_doc(qid))\n",
    "    response_embeddings = module3.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "    sim1 = np.inner(question_embeddings['outputs'], response_embeddings['outputs']).ravel()\n",
    "    questions = [tr_queries[qid]]\n",
    "    responses = list(generator_titles_tr(queries_doc_map_all[qid]))\n",
    "    response_contexts = list(generator_titles_tr(queries_doc_map_all[qid]))\n",
    "    question_embeddings = module4.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "    response_embeddings = module4.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "    sim4 = np.inner(question_embeddings['outputs'], response_embeddings['outputs']).ravel()\n",
    "    sim5 = cosine(embed3([tr_queries[qid]])[0].numpy().reshape(1,-1), embed3(list(generator_titles_tr(queries_doc_map_all[qid]))).numpy()).ravel()\n",
    "    return np.hstack((sim1.reshape(-1,1), sim4.reshape(-1,1), sim5.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
